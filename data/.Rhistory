#this is the overall, general error, which just distributes across all items and subjects uniformly (i.e., there's no structure)
error <- rnorm(mean=0, sd=overall_noise, n=number_of_items_per_condition*2*number_of_sbj);
#this actually generates the simulated values
estimated_probs <- (across_language+sbj_intercept) + (within_language+sbj_slope)*cross_linguistic + error;
estimated_probs <- sapply(estimated_probs, function(a) min(max(a,0),1)); #this limits probabilities between 0 and 1
#estimated_choices <- round(estimated_probs, digits=0); #this is the deterministic way to transform a probabilty into a binary choice (anything up to .50 becomes a zero, anything .51 or above becomes a one)
estimated_choices <- rbinom(n=length(estimated_probs), size=1, prob=estimated_probs); #this probably makes more sense; it passes a probability value into a generator of ones/zeros
#this merely put everything together in a dataframe that the 'glmer' function below will like
simulated_data <- data.frame(estimated_choices=estimated_choices, cross_linguistic=cross_linguistic, sbj_id=rep(seq(1:number_of_sbj), each=number_of_items_per_condition*2));
out_datasets[[i]] <- simulated_data;
#this is the modelling part, which is essentially reverse-engineering the data, that is, it's trying to estimate the ground-truth parameters we set above (across_language_parameter and, more importantly, within_language_parameter) from the simulated data (which is not so banal, cause we injected noise in the data -- like the harsh, evil real world will do with our data)
model <- glmer(estimated_choices ~ cross_linguistic + (1|sbj_id) + (0+cross_linguistic|sbj_id), data=simulated_data, family='binomial');
#this is writing out to our dummy variable the metric of interest from the model, which we'll then use to compute power
out_p_values_mixed_model[i] <- summary(model)[[10]][2,4];
#this prepares the dataset for a t test, which needs aggregated data
data_for_t_test <- aggregate(estimated_choices ~ cross_linguistic + sbj_id, FUN=mean, data=simulated_data);
#this runs the t test
temp <- t.test(data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==0], data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==1], paired=T, alternative='less');
#and again, this is just writing the metric of interest into our dummy, output-collection variable
out_p_values_t_test[i] <- temp[[3]];
print(i);
rm(model, simulated_data, sbj_intercept, sbj_slope, error, estimated_probs, estimated_choices, data_for_t_test, temp);
}
rm(across_language, within_language, cross_linguistic, i);
summary(out_p_values_mixed_model<.05);
summary(out_p_values_t_test<.05)
rm(list=ls()); #this clears the workspace
library(lme4);
n_sim <- 500; #this is the number of simulations that we will run
number_of_items_per_condition <- 20;
number_of_sbj <- 20;
across_language_parameter <- .45; #this would be the expected proportion of YES responses on items with stems from language 1 and affixes from language 2 (or vice versa)
within_language_parameter <- .10; #this would be the difference between the proportion of YES responses on across-language items and items that instead join together stems and affixes from the *same* language (within language). Becase we settled on .45 for across-language items, and this parameter is +0.10, this is equivalent to saying that we expect 0.45 + 0.10 = 0.55 YES responses on within-language items
sbj_intercept_variability <- .10;
sbj_slope_variability <- .10;
overall_noise <- .15;
#this generates the vectors for the parameters above, just making them of the right dimension and format to go into the simulated dataframe (see line x below)
across_language <- rep(across_language_parameter, number_of_items_per_condition*number_of_sbj);
within_language <- rep(within_language_parameter, number_of_items_per_condition*number_of_sbj);
cross_linguistic <- rep(rep(c(0,1), each=number_of_items_per_condition), number_of_sbj);
#this simply generates the vectors that will collect the relevant numbers from the simulations, one for the mixed modelling and one for the t test
out_p_values_mixed_model <- vector(length=n_sim);
out_p_values_t_test <- vector(length=n_sim);
out_datasets <- list();
#and finally, now, to the real game :-)
for (i in 1:n_sim)
{
#this generates the participants' features for this particular round of simulations
sbj_intercept <- rep(rnorm(mean=0, sd=sbj_intercept_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
sbj_slope <- rep(rnorm(mean=0, sd=sbj_slope_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
#this is the overall, general error, which just distributes across all items and subjects uniformly (i.e., there's no structure)
error <- rnorm(mean=0, sd=overall_noise, n=number_of_items_per_condition*2*number_of_sbj);
#this actually generates the simulated values
estimated_probs <- (across_language+sbj_intercept) + (within_language+sbj_slope)*cross_linguistic + error;
estimated_probs <- sapply(estimated_probs, function(a) min(max(a,0),1)); #this limits probabilities between 0 and 1
#estimated_choices <- round(estimated_probs, digits=0); #this is the deterministic way to transform a probabilty into a binary choice (anything up to .50 becomes a zero, anything .51 or above becomes a one)
estimated_choices <- rbinom(n=length(estimated_probs), size=1, prob=estimated_probs); #this probably makes more sense; it passes a probability value into a generator of ones/zeros
#this merely put everything together in a dataframe that the 'glmer' function below will like
simulated_data <- data.frame(estimated_choices=estimated_choices, cross_linguistic=cross_linguistic, sbj_id=rep(seq(1:number_of_sbj), each=number_of_items_per_condition*2));
out_datasets[[i]] <- simulated_data;
#this is the modelling part, which is essentially reverse-engineering the data, that is, it's trying to estimate the ground-truth parameters we set above (across_language_parameter and, more importantly, within_language_parameter) from the simulated data (which is not so banal, cause we injected noise in the data -- like the harsh, evil real world will do with our data)
model <- glmer(estimated_choices ~ cross_linguistic + (1|sbj_id) + (0+cross_linguistic|sbj_id), data=simulated_data, family='binomial');
#this is writing out to our dummy variable the metric of interest from the model, which we'll then use to compute power
out_p_values_mixed_model[i] <- summary(model)[[10]][2,4];
#this prepares the dataset for a t test, which needs aggregated data
data_for_t_test <- aggregate(estimated_choices ~ cross_linguistic + sbj_id, FUN=mean, data=simulated_data);
#this runs the t test
temp <- t.test(data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==0], data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==1], paired=T, alternative='less');
#and again, this is just writing the metric of interest into our dummy, output-collection variable
out_p_values_t_test[i] <- temp[[3]];
print(i);
rm(model, simulated_data, sbj_intercept, sbj_slope, error, estimated_probs, estimated_choices, data_for_t_test, temp);
}
rm(across_language, within_language, cross_linguistic, i);
summary(out_p_values_mixed_model<.05);
summary(out_p_values_t_test<.05)
rm(list=ls()); #this clears the workspace
n_sim <- 500; #this is the number of simulations that we will run
number_of_items_per_condition <- 20;
number_of_sbj <- 50;
across_language_parameter <- .45; #this would be the expected proportion of YES responses on items with stems from language 1 and affixes from language 2 (or vice versa)
within_language_parameter <- .10; #this would be the difference between the proportion of YES responses on across-language items and items that instead join together stems and affixes from the *same* language (within language). Becase we settled on .45 for across-language items, and this parameter is +0.10, this is equivalent to saying that we expect 0.45 + 0.10 = 0.55 YES responses on within-language items
sbj_intercept_variability <- .10;
sbj_slope_variability <- .10;
overall_noise <- .15;
#this generates the vectors for the parameters above, just making them of the right dimension and format to go into the simulated dataframe (see line x below)
across_language <- rep(across_language_parameter, number_of_items_per_condition*number_of_sbj);
within_language <- rep(within_language_parameter, number_of_items_per_condition*number_of_sbj);
cross_linguistic <- rep(rep(c(0,1), each=number_of_items_per_condition), number_of_sbj);
#this simply generates the vectors that will collect the relevant numbers from the simulations, one for the mixed modelling and one for the t test
out_p_values_mixed_model <- vector(length=n_sim);
out_p_values_t_test <- vector(length=n_sim);
out_datasets <- list();
#and finally, now, to the real game :-)
for (i in 1:n_sim)
{
#this generates the participants' features for this particular round of simulations
sbj_intercept <- rep(rnorm(mean=0, sd=sbj_intercept_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
sbj_slope <- rep(rnorm(mean=0, sd=sbj_slope_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
#this is the overall, general error, which just distributes across all items and subjects uniformly (i.e., there's no structure)
error <- rnorm(mean=0, sd=overall_noise, n=number_of_items_per_condition*2*number_of_sbj);
#this actually generates the simulated values
estimated_probs <- (across_language+sbj_intercept) + (within_language+sbj_slope)*cross_linguistic + error;
estimated_probs <- sapply(estimated_probs, function(a) min(max(a,0),1)); #this limits probabilities between 0 and 1
#estimated_choices <- round(estimated_probs, digits=0); #this is the deterministic way to transform a probabilty into a binary choice (anything up to .50 becomes a zero, anything .51 or above becomes a one)
estimated_choices <- rbinom(n=length(estimated_probs), size=1, prob=estimated_probs); #this probably makes more sense; it passes a probability value into a generator of ones/zeros
#this merely put everything together in a dataframe that the 'glmer' function below will like
simulated_data <- data.frame(estimated_choices=estimated_choices, cross_linguistic=cross_linguistic, sbj_id=rep(seq(1:number_of_sbj), each=number_of_items_per_condition*2));
out_datasets[[i]] <- simulated_data;
#this is the modelling part, which is essentially reverse-engineering the data, that is, it's trying to estimate the ground-truth parameters we set above (across_language_parameter and, more importantly, within_language_parameter) from the simulated data (which is not so banal, cause we injected noise in the data -- like the harsh, evil real world will do with our data)
model <- glmer(estimated_choices ~ cross_linguistic + (1|sbj_id) + (0+cross_linguistic|sbj_id), data=simulated_data, family='binomial');
#this is writing out to our dummy variable the metric of interest from the model, which we'll then use to compute power
out_p_values_mixed_model[i] <- summary(model)[[10]][2,4];
#this prepares the dataset for a t test, which needs aggregated data
data_for_t_test <- aggregate(estimated_choices ~ cross_linguistic + sbj_id, FUN=mean, data=simulated_data);
#this runs the t test
temp <- t.test(data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==0], data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==1], paired=T, alternative='less');
#and again, this is just writing the metric of interest into our dummy, output-collection variable
out_p_values_t_test[i] <- temp[[3]];
print(i);
rm(model, simulated_data, sbj_intercept, sbj_slope, error, estimated_probs, estimated_choices, data_for_t_test, temp);
}
rm(across_language, within_language, cross_linguistic, i);
summary(out_p_values_mixed_model<.05);
summary(out_p_values_t_test<.05)
48*2
rm(list=ls()); #this clears the workspace
library(lme4);
n_sim <- 500; #this is the number of simulations that we will run
number_of_items_per_condition <- 20;
number_of_sbj <- 50;
across_language_parameter <- .50; #this would be the expected proportion of YES responses on items with stems from language 1 and affixes from language 2 (or vice versa)
within_language_parameter <- .05; #this would be the difference between the proportion of YES responses on across-language items and items that instead join together stems and affixes from the *same* language (within language). Becase we settled on .45 for across-language items, and this parameter is +0.10, this is equivalent to saying that we expect 0.45 + 0.10 = 0.55 YES responses on within-language items
sbj_intercept_variability <- .10;
sbj_slope_variability <- .10;
overall_noise <- .15;
#this generates the vectors for the parameters above, just making them of the right dimension and format to go into the simulated dataframe (see line x below)
across_language <- rep(across_language_parameter, number_of_items_per_condition*number_of_sbj);
within_language <- rep(within_language_parameter, number_of_items_per_condition*number_of_sbj);
cross_linguistic <- rep(rep(c(0,1), each=number_of_items_per_condition), number_of_sbj);
#this simply generates the vectors that will collect the relevant numbers from the simulations, one for the mixed modelling and one for the t test
out_p_values_mixed_model <- vector(length=n_sim);
out_p_values_t_test <- vector(length=n_sim);
out_datasets <- list();
#and finally, now, to the real game :-)
for (i in 1:n_sim)
{
#this generates the participants' features for this particular round of simulations
sbj_intercept <- rep(rnorm(mean=0, sd=sbj_intercept_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
sbj_slope <- rep(rnorm(mean=0, sd=sbj_slope_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
#this is the overall, general error, which just distributes across all items and subjects uniformly (i.e., there's no structure)
error <- rnorm(mean=0, sd=overall_noise, n=number_of_items_per_condition*2*number_of_sbj);
#this actually generates the simulated values
estimated_probs <- (across_language+sbj_intercept) + (within_language+sbj_slope)*cross_linguistic + error;
estimated_probs <- sapply(estimated_probs, function(a) min(max(a,0),1)); #this limits probabilities between 0 and 1
#estimated_choices <- round(estimated_probs, digits=0); #this is the deterministic way to transform a probabilty into a binary choice (anything up to .50 becomes a zero, anything .51 or above becomes a one)
estimated_choices <- rbinom(n=length(estimated_probs), size=1, prob=estimated_probs); #this probably makes more sense; it passes a probability value into a generator of ones/zeros
#this merely put everything together in a dataframe that the 'glmer' function below will like
simulated_data <- data.frame(estimated_choices=estimated_choices, cross_linguistic=cross_linguistic, sbj_id=rep(seq(1:number_of_sbj), each=number_of_items_per_condition*2));
out_datasets[[i]] <- simulated_data;
#this is the modelling part, which is essentially reverse-engineering the data, that is, it's trying to estimate the ground-truth parameters we set above (across_language_parameter and, more importantly, within_language_parameter) from the simulated data (which is not so banal, cause we injected noise in the data -- like the harsh, evil real world will do with our data)
model <- glmer(estimated_choices ~ cross_linguistic + (1|sbj_id) + (0+cross_linguistic|sbj_id), data=simulated_data, family='binomial');
#this is writing out to our dummy variable the metric of interest from the model, which we'll then use to compute power
out_p_values_mixed_model[i] <- summary(model)[[10]][2,4];
#this prepares the dataset for a t test, which needs aggregated data
data_for_t_test <- aggregate(estimated_choices ~ cross_linguistic + sbj_id, FUN=mean, data=simulated_data);
#this runs the t test
temp <- t.test(data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==0], data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==1], paired=T, alternative='less');
#and again, this is just writing the metric of interest into our dummy, output-collection variable
out_p_values_t_test[i] <- temp[[3]];
print(i);
rm(model, simulated_data, sbj_intercept, sbj_slope, error, estimated_probs, estimated_choices, data_for_t_test, temp);
}
rm(across_language, within_language, cross_linguistic, i);
summary(out_p_values_mixed_model<.05);
summary(out_p_values_t_test<.05)
25.8*2
rm(list=ls()); #this clears the workspace
n_sim <- 500; #this is the number of simulations that we will run
number_of_items_per_condition <- 20;
number_of_sbj <- 100;
across_language_parameter <- .50; #this would be the expected proportion of YES responses on items with stems from language 1 and affixes from language 2 (or vice versa)
within_language_parameter <- .05; #this would be the difference between the proportion of YES responses on across-language items and items that instead join together stems and affixes from the *same* language (within language). Becase we settled on .45 for across-language items, and this parameter is +0.10, this is equivalent to saying that we expect 0.45 + 0.10 = 0.55 YES responses on within-language items
sbj_intercept_variability <- .10;
sbj_slope_variability <- .10;
overall_noise <- .15;
#this generates the vectors for the parameters above, just making them of the right dimension and format to go into the simulated dataframe (see line x below)
across_language <- rep(across_language_parameter, number_of_items_per_condition*number_of_sbj);
within_language <- rep(within_language_parameter, number_of_items_per_condition*number_of_sbj);
cross_linguistic <- rep(rep(c(0,1), each=number_of_items_per_condition), number_of_sbj);
#this simply generates the vectors that will collect the relevant numbers from the simulations, one for the mixed modelling and one for the t test
out_p_values_mixed_model <- vector(length=n_sim);
out_p_values_t_test <- vector(length=n_sim);
out_datasets <- list();
#and finally, now, to the real game :-)
for (i in 1:n_sim)
{
#this generates the participants' features for this particular round of simulations
sbj_intercept <- rep(rnorm(mean=0, sd=sbj_intercept_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
sbj_slope <- rep(rnorm(mean=0, sd=sbj_slope_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
#this is the overall, general error, which just distributes across all items and subjects uniformly (i.e., there's no structure)
error <- rnorm(mean=0, sd=overall_noise, n=number_of_items_per_condition*2*number_of_sbj);
#this actually generates the simulated values
estimated_probs <- (across_language+sbj_intercept) + (within_language+sbj_slope)*cross_linguistic + error;
estimated_probs <- sapply(estimated_probs, function(a) min(max(a,0),1)); #this limits probabilities between 0 and 1
#estimated_choices <- round(estimated_probs, digits=0); #this is the deterministic way to transform a probabilty into a binary choice (anything up to .50 becomes a zero, anything .51 or above becomes a one)
estimated_choices <- rbinom(n=length(estimated_probs), size=1, prob=estimated_probs); #this probably makes more sense; it passes a probability value into a generator of ones/zeros
#this merely put everything together in a dataframe that the 'glmer' function below will like
simulated_data <- data.frame(estimated_choices=estimated_choices, cross_linguistic=cross_linguistic, sbj_id=rep(seq(1:number_of_sbj), each=number_of_items_per_condition*2));
out_datasets[[i]] <- simulated_data;
#this is the modelling part, which is essentially reverse-engineering the data, that is, it's trying to estimate the ground-truth parameters we set above (across_language_parameter and, more importantly, within_language_parameter) from the simulated data (which is not so banal, cause we injected noise in the data -- like the harsh, evil real world will do with our data)
model <- glmer(estimated_choices ~ cross_linguistic + (1|sbj_id) + (0+cross_linguistic|sbj_id), data=simulated_data, family='binomial');
#this is writing out to our dummy variable the metric of interest from the model, which we'll then use to compute power
out_p_values_mixed_model[i] <- summary(model)[[10]][2,4];
#this prepares the dataset for a t test, which needs aggregated data
data_for_t_test <- aggregate(estimated_choices ~ cross_linguistic + sbj_id, FUN=mean, data=simulated_data);
#this runs the t test
temp <- t.test(data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==0], data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==1], paired=T, alternative='less');
#and again, this is just writing the metric of interest into our dummy, output-collection variable
out_p_values_t_test[i] <- temp[[3]];
print(i);
rm(model, simulated_data, sbj_intercept, sbj_slope, error, estimated_probs, estimated_choices, data_for_t_test, temp);
}
rm(across_language, within_language, cross_linguistic, i);
summary(out_p_values_mixed_model<.05);
summary(out_p_values_t_test<.05)
39.9*2
rm(list=ls()); #this clears the workspace
n_sim <- 500; #this is the number of simulations that we will run
number_of_items_per_condition <- 20;
number_of_sbj <- 150;
across_language_parameter <- .50; #this would be the expected proportion of YES responses on items with stems from language 1 and affixes from language 2 (or vice versa)
within_language_parameter <- .05; #this would be the difference between the proportion of YES responses on across-language items and items that instead join together stems and affixes from the *same* language (within language). Becase we settled on .45 for across-language items, and this parameter is +0.10, this is equivalent to saying that we expect 0.45 + 0.10 = 0.55 YES responses on within-language items
sbj_intercept_variability <- .10;
sbj_slope_variability <- .10;
overall_noise <- .15;
#this generates the vectors for the parameters above, just making them of the right dimension and format to go into the simulated dataframe (see line x below)
across_language <- rep(across_language_parameter, number_of_items_per_condition*number_of_sbj);
within_language <- rep(within_language_parameter, number_of_items_per_condition*number_of_sbj);
cross_linguistic <- rep(rep(c(0,1), each=number_of_items_per_condition), number_of_sbj);
#this simply generates the vectors that will collect the relevant numbers from the simulations, one for the mixed modelling and one for the t test
out_p_values_mixed_model <- vector(length=n_sim);
out_p_values_t_test <- vector(length=n_sim);
out_datasets <- list();
#and finally, now, to the real game :-)
for (i in 1:n_sim)
{
#this generates the participants' features for this particular round of simulations
sbj_intercept <- rep(rnorm(mean=0, sd=sbj_intercept_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
sbj_slope <- rep(rnorm(mean=0, sd=sbj_slope_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
#this is the overall, general error, which just distributes across all items and subjects uniformly (i.e., there's no structure)
error <- rnorm(mean=0, sd=overall_noise, n=number_of_items_per_condition*2*number_of_sbj);
#this actually generates the simulated values
estimated_probs <- (across_language+sbj_intercept) + (within_language+sbj_slope)*cross_linguistic + error;
estimated_probs <- sapply(estimated_probs, function(a) min(max(a,0),1)); #this limits probabilities between 0 and 1
#estimated_choices <- round(estimated_probs, digits=0); #this is the deterministic way to transform a probabilty into a binary choice (anything up to .50 becomes a zero, anything .51 or above becomes a one)
estimated_choices <- rbinom(n=length(estimated_probs), size=1, prob=estimated_probs); #this probably makes more sense; it passes a probability value into a generator of ones/zeros
#this merely put everything together in a dataframe that the 'glmer' function below will like
simulated_data <- data.frame(estimated_choices=estimated_choices, cross_linguistic=cross_linguistic, sbj_id=rep(seq(1:number_of_sbj), each=number_of_items_per_condition*2));
out_datasets[[i]] <- simulated_data;
#this is the modelling part, which is essentially reverse-engineering the data, that is, it's trying to estimate the ground-truth parameters we set above (across_language_parameter and, more importantly, within_language_parameter) from the simulated data (which is not so banal, cause we injected noise in the data -- like the harsh, evil real world will do with our data)
model <- glmer(estimated_choices ~ cross_linguistic + (1|sbj_id) + (0+cross_linguistic|sbj_id), data=simulated_data, family='binomial');
#this is writing out to our dummy variable the metric of interest from the model, which we'll then use to compute power
out_p_values_mixed_model[i] <- summary(model)[[10]][2,4];
#this prepares the dataset for a t test, which needs aggregated data
data_for_t_test <- aggregate(estimated_choices ~ cross_linguistic + sbj_id, FUN=mean, data=simulated_data);
#this runs the t test
temp <- t.test(data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==0], data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==1], paired=T, alternative='less');
#and again, this is just writing the metric of interest into our dummy, output-collection variable
out_p_values_t_test[i] <- temp[[3]];
print(i);
rm(model, simulated_data, sbj_intercept, sbj_slope, error, estimated_probs, estimated_choices, data_for_t_test, temp);
}
rm(across_language, within_language, cross_linguistic, i);
summary(out_p_values_mixed_model<.05);
summary(out_p_values_t_test<.05)
45.8*2
rm(list=ls()); #this clears the workspace
n_sim <- 500; #this is the number of simulations that we will run
number_of_items_per_condition <- 20;
number_of_sbj <- 200;
across_language_parameter <- .50; #this would be the expected proportion of YES responses on items with stems from language 1 and affixes from language 2 (or vice versa)
within_language_parameter <- .05; #this would be the difference between the proportion of YES responses on across-language items and items that instead join together stems and affixes from the *same* language (within language). Becase we settled on .45 for across-language items, and this parameter is +0.10, this is equivalent to saying that we expect 0.45 + 0.10 = 0.55 YES responses on within-language items
sbj_intercept_variability <- .10;
sbj_slope_variability <- .10;
overall_noise <- .15;
#this generates the vectors for the parameters above, just making them of the right dimension and format to go into the simulated dataframe (see line x below)
across_language <- rep(across_language_parameter, number_of_items_per_condition*number_of_sbj);
within_language <- rep(within_language_parameter, number_of_items_per_condition*number_of_sbj);
cross_linguistic <- rep(rep(c(0,1), each=number_of_items_per_condition), number_of_sbj);
#this simply generates the vectors that will collect the relevant numbers from the simulations, one for the mixed modelling and one for the t test
out_p_values_mixed_model <- vector(length=n_sim);
out_p_values_t_test <- vector(length=n_sim);
out_datasets <- list();
#and finally, now, to the real game :-)
for (i in 1:n_sim)
{
#this generates the participants' features for this particular round of simulations
sbj_intercept <- rep(rnorm(mean=0, sd=sbj_intercept_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
sbj_slope <- rep(rnorm(mean=0, sd=sbj_slope_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
#this is the overall, general error, which just distributes across all items and subjects uniformly (i.e., there's no structure)
error <- rnorm(mean=0, sd=overall_noise, n=number_of_items_per_condition*2*number_of_sbj);
#this actually generates the simulated values
estimated_probs <- (across_language+sbj_intercept) + (within_language+sbj_slope)*cross_linguistic + error;
estimated_probs <- sapply(estimated_probs, function(a) min(max(a,0),1)); #this limits probabilities between 0 and 1
#estimated_choices <- round(estimated_probs, digits=0); #this is the deterministic way to transform a probabilty into a binary choice (anything up to .50 becomes a zero, anything .51 or above becomes a one)
estimated_choices <- rbinom(n=length(estimated_probs), size=1, prob=estimated_probs); #this probably makes more sense; it passes a probability value into a generator of ones/zeros
#this merely put everything together in a dataframe that the 'glmer' function below will like
simulated_data <- data.frame(estimated_choices=estimated_choices, cross_linguistic=cross_linguistic, sbj_id=rep(seq(1:number_of_sbj), each=number_of_items_per_condition*2));
out_datasets[[i]] <- simulated_data;
#this is the modelling part, which is essentially reverse-engineering the data, that is, it's trying to estimate the ground-truth parameters we set above (across_language_parameter and, more importantly, within_language_parameter) from the simulated data (which is not so banal, cause we injected noise in the data -- like the harsh, evil real world will do with our data)
model <- glmer(estimated_choices ~ cross_linguistic + (1|sbj_id) + (0+cross_linguistic|sbj_id), data=simulated_data, family='binomial');
#this is writing out to our dummy variable the metric of interest from the model, which we'll then use to compute power
out_p_values_mixed_model[i] <- summary(model)[[10]][2,4];
#this prepares the dataset for a t test, which needs aggregated data
data_for_t_test <- aggregate(estimated_choices ~ cross_linguistic + sbj_id, FUN=mean, data=simulated_data);
#this runs the t test
temp <- t.test(data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==0], data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==1], paired=T, alternative='less');
#and again, this is just writing the metric of interest into our dummy, output-collection variable
out_p_values_t_test[i] <- temp[[3]];
print(i);
rm(model, simulated_data, sbj_intercept, sbj_slope, error, estimated_probs, estimated_choices, data_for_t_test, temp);
}
rm(across_language, within_language, cross_linguistic, i);
summary(out_p_values_mixed_model<.05);
summary(out_p_values_t_test<.05)
48.1*2
rm(list=ls()); #this clears the workspace
n_sim <- 500; #this is the number of simulations that we will run
number_of_items_per_condition <- 20;
number_of_sbj <- 175;
across_language_parameter <- .50; #this would be the expected proportion of YES responses on items with stems from language 1 and affixes from language 2 (or vice versa)
within_language_parameter <- .05; #this would be the difference between the proportion of YES responses on across-language items and items that instead join together stems and affixes from the *same* language (within language). Becase we settled on .45 for across-language items, and this parameter is +0.10, this is equivalent to saying that we expect 0.45 + 0.10 = 0.55 YES responses on within-language items
sbj_intercept_variability <- .10;
sbj_slope_variability <- .10;
overall_noise <- .15;
#this generates the vectors for the parameters above, just making them of the right dimension and format to go into the simulated dataframe (see line x below)
across_language <- rep(across_language_parameter, number_of_items_per_condition*number_of_sbj);
within_language <- rep(within_language_parameter, number_of_items_per_condition*number_of_sbj);
cross_linguistic <- rep(rep(c(0,1), each=number_of_items_per_condition), number_of_sbj);
#this simply generates the vectors that will collect the relevant numbers from the simulations, one for the mixed modelling and one for the t test
out_p_values_mixed_model <- vector(length=n_sim);
out_p_values_t_test <- vector(length=n_sim);
out_datasets <- list();
#and finally, now, to the real game :-)
for (i in 1:n_sim)
{
#this generates the participants' features for this particular round of simulations
sbj_intercept <- rep(rnorm(mean=0, sd=sbj_intercept_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
sbj_slope <- rep(rnorm(mean=0, sd=sbj_slope_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
#this is the overall, general error, which just distributes across all items and subjects uniformly (i.e., there's no structure)
error <- rnorm(mean=0, sd=overall_noise, n=number_of_items_per_condition*2*number_of_sbj);
#this actually generates the simulated values
estimated_probs <- (across_language+sbj_intercept) + (within_language+sbj_slope)*cross_linguistic + error;
estimated_probs <- sapply(estimated_probs, function(a) min(max(a,0),1)); #this limits probabilities between 0 and 1
#estimated_choices <- round(estimated_probs, digits=0); #this is the deterministic way to transform a probabilty into a binary choice (anything up to .50 becomes a zero, anything .51 or above becomes a one)
estimated_choices <- rbinom(n=length(estimated_probs), size=1, prob=estimated_probs); #this probably makes more sense; it passes a probability value into a generator of ones/zeros
#this merely put everything together in a dataframe that the 'glmer' function below will like
simulated_data <- data.frame(estimated_choices=estimated_choices, cross_linguistic=cross_linguistic, sbj_id=rep(seq(1:number_of_sbj), each=number_of_items_per_condition*2));
out_datasets[[i]] <- simulated_data;
#this is the modelling part, which is essentially reverse-engineering the data, that is, it's trying to estimate the ground-truth parameters we set above (across_language_parameter and, more importantly, within_language_parameter) from the simulated data (which is not so banal, cause we injected noise in the data -- like the harsh, evil real world will do with our data)
model <- glmer(estimated_choices ~ cross_linguistic + (1|sbj_id) + (0+cross_linguistic|sbj_id), data=simulated_data, family='binomial');
#this is writing out to our dummy variable the metric of interest from the model, which we'll then use to compute power
out_p_values_mixed_model[i] <- summary(model)[[10]][2,4];
#this prepares the dataset for a t test, which needs aggregated data
data_for_t_test <- aggregate(estimated_choices ~ cross_linguistic + sbj_id, FUN=mean, data=simulated_data);
#this runs the t test
temp <- t.test(data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==0], data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==1], paired=T, alternative='less');
#and again, this is just writing the metric of interest into our dummy, output-collection variable
out_p_values_t_test[i] <- temp[[3]];
print(i);
rm(model, simulated_data, sbj_intercept, sbj_slope, error, estimated_probs, estimated_choices, data_for_t_test, temp);
}
rm(across_language, within_language, cross_linguistic, i);
summary(out_p_values_mixed_model<.05);
summary(out_p_values_t_test<.05)
47.2*2
rm(list=ls()); #this clears the workspace
n_sim <- 500; #this is the number of simulations that we will run
number_of_items_per_condition <- 20;
number_of_sbj <- 185;
across_language_parameter <- .50; #this would be the expected proportion of YES responses on items with stems from language 1 and affixes from language 2 (or vice versa)
within_language_parameter <- .05; #this would be the difference between the proportion of YES responses on across-language items and items that instead join together stems and affixes from the *same* language (within language). Becase we settled on .45 for across-language items, and this parameter is +0.10, this is equivalent to saying that we expect 0.45 + 0.10 = 0.55 YES responses on within-language items
sbj_intercept_variability <- .10;
sbj_slope_variability <- .10;
overall_noise <- .15;
#this generates the vectors for the parameters above, just making them of the right dimension and format to go into the simulated dataframe (see line x below)
across_language <- rep(across_language_parameter, number_of_items_per_condition*number_of_sbj);
within_language <- rep(within_language_parameter, number_of_items_per_condition*number_of_sbj);
cross_linguistic <- rep(rep(c(0,1), each=number_of_items_per_condition), number_of_sbj);
#this simply generates the vectors that will collect the relevant numbers from the simulations, one for the mixed modelling and one for the t test
out_p_values_mixed_model <- vector(length=n_sim);
out_p_values_t_test <- vector(length=n_sim);
out_datasets <- list();
#and finally, now, to the real game :-)
for (i in 1:n_sim)
{
#this generates the participants' features for this particular round of simulations
sbj_intercept <- rep(rnorm(mean=0, sd=sbj_intercept_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
sbj_slope <- rep(rnorm(mean=0, sd=sbj_slope_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
#this is the overall, general error, which just distributes across all items and subjects uniformly (i.e., there's no structure)
error <- rnorm(mean=0, sd=overall_noise, n=number_of_items_per_condition*2*number_of_sbj);
#this actually generates the simulated values
estimated_probs <- (across_language+sbj_intercept) + (within_language+sbj_slope)*cross_linguistic + error;
estimated_probs <- sapply(estimated_probs, function(a) min(max(a,0),1)); #this limits probabilities between 0 and 1
#estimated_choices <- round(estimated_probs, digits=0); #this is the deterministic way to transform a probabilty into a binary choice (anything up to .50 becomes a zero, anything .51 or above becomes a one)
estimated_choices <- rbinom(n=length(estimated_probs), size=1, prob=estimated_probs); #this probably makes more sense; it passes a probability value into a generator of ones/zeros
#this merely put everything together in a dataframe that the 'glmer' function below will like
simulated_data <- data.frame(estimated_choices=estimated_choices, cross_linguistic=cross_linguistic, sbj_id=rep(seq(1:number_of_sbj), each=number_of_items_per_condition*2));
out_datasets[[i]] <- simulated_data;
#this is the modelling part, which is essentially reverse-engineering the data, that is, it's trying to estimate the ground-truth parameters we set above (across_language_parameter and, more importantly, within_language_parameter) from the simulated data (which is not so banal, cause we injected noise in the data -- like the harsh, evil real world will do with our data)
model <- glmer(estimated_choices ~ cross_linguistic + (1|sbj_id) + (0+cross_linguistic|sbj_id), data=simulated_data, family='binomial');
#this is writing out to our dummy variable the metric of interest from the model, which we'll then use to compute power
out_p_values_mixed_model[i] <- summary(model)[[10]][2,4];
#this prepares the dataset for a t test, which needs aggregated data
data_for_t_test <- aggregate(estimated_choices ~ cross_linguistic + sbj_id, FUN=mean, data=simulated_data);
#this runs the t test
temp <- t.test(data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==0], data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==1], paired=T, alternative='less');
#and again, this is just writing the metric of interest into our dummy, output-collection variable
out_p_values_t_test[i] <- temp[[3]];
print(i);
rm(model, simulated_data, sbj_intercept, sbj_slope, error, estimated_probs, estimated_choices, data_for_t_test, temp);
}
rm(across_language, within_language, cross_linguistic, i);
summary(out_p_values_mixed_model<.05);
summary(out_p_values_t_test<.05)
48.3*2
rm(list=ls()); #this clears the workspace
n_sim <- 500; #this is the number of simulations that we will run
number_of_items_per_condition <- 20;
number_of_sbj <- 190;
across_language_parameter <- .50; #this would be the expected proportion of YES responses on items with stems from language 1 and affixes from language 2 (or vice versa)
within_language_parameter <- .05; #this would be the difference between the proportion of YES responses on across-language items and items that instead join together stems and affixes from the *same* language (within language). Becase we settled on .45 for across-language items, and this parameter is +0.10, this is equivalent to saying that we expect 0.45 + 0.10 = 0.55 YES responses on within-language items
sbj_intercept_variability <- .10;
sbj_slope_variability <- .10;
overall_noise <- .15;
#this generates the vectors for the parameters above, just making them of the right dimension and format to go into the simulated dataframe (see line x below)
across_language <- rep(across_language_parameter, number_of_items_per_condition*number_of_sbj);
within_language <- rep(within_language_parameter, number_of_items_per_condition*number_of_sbj);
cross_linguistic <- rep(rep(c(0,1), each=number_of_items_per_condition), number_of_sbj);
#this simply generates the vectors that will collect the relevant numbers from the simulations, one for the mixed modelling and one for the t test
out_p_values_mixed_model <- vector(length=n_sim);
out_p_values_t_test <- vector(length=n_sim);
out_datasets <- list();
#and finally, now, to the real game :-)
for (i in 1:n_sim)
{
#this generates the participants' features for this particular round of simulations
sbj_intercept <- rep(rnorm(mean=0, sd=sbj_intercept_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
sbj_slope <- rep(rnorm(mean=0, sd=sbj_slope_variability, n=number_of_sbj), each=number_of_items_per_condition*2);
#this is the overall, general error, which just distributes across all items and subjects uniformly (i.e., there's no structure)
error <- rnorm(mean=0, sd=overall_noise, n=number_of_items_per_condition*2*number_of_sbj);
#this actually generates the simulated values
estimated_probs <- (across_language+sbj_intercept) + (within_language+sbj_slope)*cross_linguistic + error;
estimated_probs <- sapply(estimated_probs, function(a) min(max(a,0),1)); #this limits probabilities between 0 and 1
#estimated_choices <- round(estimated_probs, digits=0); #this is the deterministic way to transform a probabilty into a binary choice (anything up to .50 becomes a zero, anything .51 or above becomes a one)
estimated_choices <- rbinom(n=length(estimated_probs), size=1, prob=estimated_probs); #this probably makes more sense; it passes a probability value into a generator of ones/zeros
#this merely put everything together in a dataframe that the 'glmer' function below will like
simulated_data <- data.frame(estimated_choices=estimated_choices, cross_linguistic=cross_linguistic, sbj_id=rep(seq(1:number_of_sbj), each=number_of_items_per_condition*2));
out_datasets[[i]] <- simulated_data;
#this is the modelling part, which is essentially reverse-engineering the data, that is, it's trying to estimate the ground-truth parameters we set above (across_language_parameter and, more importantly, within_language_parameter) from the simulated data (which is not so banal, cause we injected noise in the data -- like the harsh, evil real world will do with our data)
model <- glmer(estimated_choices ~ cross_linguistic + (1|sbj_id) + (0+cross_linguistic|sbj_id), data=simulated_data, family='binomial');
#this is writing out to our dummy variable the metric of interest from the model, which we'll then use to compute power
out_p_values_mixed_model[i] <- summary(model)[[10]][2,4];
#this prepares the dataset for a t test, which needs aggregated data
data_for_t_test <- aggregate(estimated_choices ~ cross_linguistic + sbj_id, FUN=mean, data=simulated_data);
#this runs the t test
temp <- t.test(data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==0], data_for_t_test$estimated_choices[data_for_t_test$cross_linguistic==1], paired=T, alternative='less');
#and again, this is just writing the metric of interest into our dummy, output-collection variable
out_p_values_t_test[i] <- temp[[3]];
print(i);
rm(model, simulated_data, sbj_intercept, sbj_slope, error, estimated_probs, estimated_choices, data_for_t_test, temp);
}
rm(across_language, within_language, cross_linguistic, i);
summary(out_p_values_mixed_model<.05);
summary(out_p_values_t_test<.05)
48.5*2
library(paletteer);
cols <- paletteer_d("colorBlindness::SteppedSequential5Steps");
simple_col <- "#1a8de5";
complex_col <- "#859a2f";
green_col <- "#2b725a";
library(extrafont);
par(family="Montserrat");
library(dplyr);
setwd("C:/Users/annal/OneDrive/Documents/GitHub/bilingualboundary/data");
all_ET <- read.csv("all_ET.csv",header=T,sep=",");
all_scores <- read.csv("all_scores.csv",header=T,sep=",");
all_ET <- subset(all_ET, select = -c(X,pre_target,post_target,type,TextBlock,adjusted_seq,fixations,rec_id,msgs));
all_ET$morph_type <- factor(all_ET$morph_type, levels=c("simple", "complex"));
all_ET$lst_type <- factor(all_ET$lst_type);
all_ET$target <- factor(all_ET$target);
all_ET$preview <- factor(all_ET$preview);
all_ET$trial_type <- factor(all_ET$trial_type, levels=c("identical", "cognate", "legal_nonword", "illegal_nonword"));
summary(all_ET);
all_ET$trial_issue <- factor(all_ET$trial_issue);
summary(all_ET);
all_ET <- subset(all_ET, select = -c(X,post_target,type,TextBlock,adjusted_seq,fixations,rec_id,msgs,noshorts_seq));
all_ET <- read.csv("all_ET.csv",header=T,sep=",");
summary(all_ET);
all_ET <- subset(all_ET, select = -c(X,post_target,type,TextBlock,adjusted_seq,fixations,rec_id,msgs,noshorts_seq));
all_ET$morph_type <- factor(all_ET$morph_type, levels=c("simple", "complex"));
all_ET$lst_type <- factor(all_ET$lst_type);
all_ET$target <- factor(all_ET$target);
all_ET$preview <- factor(all_ET$preview);
all_ET$trial_type <- factor(all_ET$trial_type, levels=c("identical", "cognate", "legal_nonword", "illegal_nonword"));
all_ET$trial_issue <- factor(all_ET$trial_issue);
summary(all_ET);
all_scores <- subset(all_scores, select = -c(X));
all_scores$sbj_ID <- as.factor(all_scores$sbj_ID);
all_scores$gender <- as.factor(all_scores$gender);
summary(all_scores);
